{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课时17 infoGAN代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-bd1a0d2132e5>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From E:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From E:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting E:\\SoftWare_Installing\\Pycharm\\Pycharm WorkPlace\\GAN生成对抗网络入门与实战\\data\\MNIST\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting E:\\SoftWare_Installing\\Pycharm\\Pycharm WorkPlace\\GAN生成对抗网络入门与实战\\data\\MNIST\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting E:\\SoftWare_Installing\\Pycharm\\Pycharm WorkPlace\\GAN生成对抗网络入门与实战\\data\\MNIST\\t10k-images-idx3-ubyte.gz\n",
      "Extracting E:\\SoftWare_Installing\\Pycharm\\Pycharm WorkPlace\\GAN生成对抗网络入门与实战\\data\\MNIST\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('E:\\SoftWare_Installing\\Pycharm\\Pycharm WorkPlace\\GAN生成对抗网络入门与实战\\data\\MNIST', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义模型搭建需要的各个组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接收输入\n",
    "def get_inputs(noise_dim, image_height, image_width, image_depth):\n",
    "    inputs_real = tf.placeholder(dtype=tf.float32, \n",
    "                                 shape=[None, image_height, image_width, image_depth],\n",
    "                                 name='inputs_real')\n",
    "    inputs_noise = tf.placeholder(dtype=tf.float32, \n",
    "                                  shape=[None, noise_dim],\n",
    "                                  name='inputs_noise')\n",
    "    # 这里condition_label.shape = [None, 10]是因为one_hot=True\n",
    "    # 需要注意的是infoGAN中的condition_label不像cGAN中那样直接从MNIST数据集的标签中获取condition_label了\n",
    "    # 而是我们自己后面需要生成一个condition_label，然后训练这个condition_label\n",
    "    # 使得这个condition_label与我们图像之间的互信息增强\n",
    "    # 二记：\n",
    "    # 在这里的演示代码里设置的condition_label是考虑控制图片里数字的类别的，因此这里的维度为[None, 10]\n",
    "    # 当然，infoGAN还可以添加多个condition_label，每个condition_label除了可以控制其产生的数字类别之外\n",
    "    # 还可以控制例如数字的粗细或者倾斜程度等表征形式，因此如果设置或者添加别的condition_label的时候\n",
    "    # 其维度不一定是[None, 10]，还可以是例如: [None, 50], [None, 100]等；\n",
    "    condition_label = tf.placeholder(dtype=tf.float32, shape=[None, 10], name='condition_label')\n",
    "    return inputs_real, inputs_noise, condition_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义生成器\n",
    "def generator(noise_img, output_dim, condition_label, is_train=True):\n",
    "    with tf.variable_scope(name_or_scope='generator', reuse=(not is_train)):\n",
    "        # 第一层为全连接层，将噪声数据的维度从100 x 1 ===> 4 x 4 x 512\n",
    "        # 在传入网络之前需要将noise和condition合并起来\n",
    "        noise_img_ = tf.concat(values=[noise_img, condition_label], axis=1)\n",
    "        layer_1 = tf.layers.dense(noise_img_, 4*4*512)\n",
    "        layer_1 = tf.reshape(layer_1, [-1, 4, 4, 512])\n",
    "        # batch_normalization\n",
    "        layer_1 = tf.layers.batch_normalization(layer_1, training=is_train)\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        # dropout\n",
    "        layer_1 = tf.nn.dropout(layer_1, keep_prob=0.8)\n",
    "        \n",
    "        # 4 x 4 x 512 ===> 7 x 7 x 256\n",
    "        layer_2 = tf.layers.conv2d_transpose(inputs=layer_1, filters=256, \n",
    "                                             kernel_size=4,\n",
    "                                             strides=1, padding='valid')\n",
    "        layer_2 = tf.layers.batch_normalization(layer_2, training=is_train)\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        layer_2 = tf.nn.dropout(layer_2, keep_prob=0.8)\n",
    "        \n",
    "        # 7 x 7 x 256 ===> 14 x 14 x 128\n",
    "        layer_3 = tf.layers.conv2d_transpose(inputs=layer_2, filters=128, \n",
    "                                             kernel_size=3,\n",
    "                                             strides=2, padding='same')\n",
    "        layer_3 = tf.layers.batch_normalization(layer_3, training=is_train)\n",
    "        layer_3 = tf.nn.relu(layer_3)\n",
    "        layer_3 = tf.nn.dropout(layer_3, keep_prob=0.8)\n",
    "        \n",
    "        # 14 x 14 x 128 ===> 28 x 28 x 1\n",
    "        logits = tf.layers.conv2d_transpose(inputs=layer_3, filters=output_dim,\n",
    "                                            kernel_size=3,\n",
    "                                            strides=2, padding='same')\n",
    "        outputs = tf.tanh(logits)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义判别器(info中的判别器与cGAN中的判别器是不一样的，它不再需要condition_label)\n",
    "def discriminator(inputs_img, reuse=False, alpha=0.01):\n",
    "    with tf.variable_scope(name_or_scope='discriminator', reuse=reuse):\n",
    "        # [28, 28, 1] ===> [14, 14, 128]\n",
    "        # 第一层不加BN\n",
    "        layer_1 = tf.layers.conv2d(inputs=inputs_img, filters=128, kernel_size=3,\n",
    "                                   strides=2, padding='same')\n",
    "        layer_1 = tf.maximum(alpha*layer_1, layer_1)\n",
    "        layer_1 = tf.nn.dropout(layer_1, keep_prob=0.8)\n",
    "        \n",
    "        # [14, 14, 128] ===> [7, 7, 256]\n",
    "        layer_2 = tf.layers.conv2d(inputs=layer_1, filters=256, kernel_size=3,\n",
    "                                   strides=2, padding='same')\n",
    "        layer_2 = tf.layers.batch_normalization(layer_2, training=True)\n",
    "        layer_2 = tf.maximum(alpha*layer_2, layer_2)\n",
    "        layer_2 = tf.nn.dropout(layer_2, keep_prob=0.8)\n",
    "        \n",
    "        # [7, 7, 256] ===> [4, 4, 512]\n",
    "        layer_3 = tf.layers.conv2d(inputs=layer_2, filters=512, kernel_size=3,\n",
    "                                   strides=2, padding='same')\n",
    "        layer_3 = tf.layers.batch_normalization(layer_3, training=True)\n",
    "        layer_3 = tf.maximum(alpha*layer_3, layer_3)\n",
    "        layer_3 = tf.nn.dropout(layer_3, keep_prob=0.8)\n",
    "        \n",
    "        # [4, 4, 512] ===> [4*4*512, 1]\n",
    "        flatten = tf.reshape(layer_3, (-1, 4*4*512))\n",
    "        logits = tf.layers.dense(flatten, 1)\n",
    "        outputs = tf.sigmoid(logits)\n",
    "        \n",
    "        return logits, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里是infoGAN的独特之处，还需要定义一个Q网络\n",
    "# infoGAN中的Q网络目的上是为了通过训练，以增强图像与condition_label之间的互信息\n",
    "# 在操作上Q网络也是一个判别器，它通过输入生成器生成的图像来判断这个图像的condition_label是否一致\n",
    "# Q网络由于还是一个判别器，所以可以考虑和判别器进行合并，最后判别器多输出就好了\n",
    "def get_Q(g_out, reuse=False, alpha=0.01):\n",
    "    with tf.variable_scope(\"Q\", reuse=reuse):\n",
    "        # 28 x 28 x 1 to 14 x 14 x 128\n",
    "        # 第一层不加入BN\n",
    "        layer1 = tf.layers.conv2d(g_out, 128, 3, strides=2, padding='same')\n",
    "        layer1 = tf.maximum(alpha * layer1, layer1)\n",
    "        layer1 = tf.nn.dropout(layer1, keep_prob=0.8)\n",
    "        \n",
    "        # 14 x 14 x 128 to 7 x 7 x 256\n",
    "        layer2 = tf.layers.conv2d(layer1, 256, 3, strides=2, padding='same')\n",
    "        layer2 = tf.layers.batch_normalization(layer2, training=True)\n",
    "        layer2 = tf.maximum(alpha * layer2, layer2)\n",
    "        layer2 = tf.nn.dropout(layer2, keep_prob=0.8)\n",
    "        \n",
    "        # 7 x 7 x 256 to 4 x 4 x 512\n",
    "        layer3 = tf.layers.conv2d(layer2, 512, 3, strides=2, padding='same')\n",
    "        layer3 = tf.layers.batch_normalization(layer3, training=True)\n",
    "        layer3 = tf.maximum(alpha * layer3, layer3)\n",
    "        layer3 = tf.nn.dropout(layer3, keep_prob=0.8)\n",
    "        \n",
    "        # 4 x 4 x 512 to 4*4*512 x 1\n",
    "        flatten = tf.reshape(layer3, (-1, 4*4*512))\n",
    "        \n",
    "        logits = tf.layers.dense(flatten, 10)\n",
    "        # 在这里的演示代码里设置的condition_label是考虑控制图片里数字的类别的，因此这里的维度为[None, 10]\n",
    "        # 因此这里的激活函数是softmax多分类激活函数\n",
    "        outputs = tf.nn.softmax(logits)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取loss值\n",
    "def get_loss(inputs_real, inputs_noise, condition_label, image_depth, smooth=0.1):\n",
    "    g_outputs = generator(inputs_noise, image_depth, condition_label, is_train=True)\n",
    "    d_logits_real, d_outputs_real = discriminator(inputs_real)\n",
    "    d_logits_fake, d_outputs_fake = discriminator(g_outputs, reuse=True)\n",
    "    q_c = get_Q(g_outputs)\n",
    "    \n",
    "    # 计算Loss\n",
    "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, \n",
    "                                                                    labels=tf.ones_like(d_outputs_fake)*(1-smooth)))\n",
    "    \n",
    "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n",
    "                                                                         labels=tf.ones_like(d_outputs_real)*(1-smooth)))\n",
    "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                                         labels=tf.zeros_like(d_outputs_fake)))\n",
    "    \n",
    "    d_loss = tf.add(d_loss_real, d_loss_fake)\n",
    "    # Q网络期望的是它判别出的图片的标签值与我们输入的condition_label是一样的\n",
    "    # 由此可以看出这个损失本质也就是一个分类损失，也就是交叉熵损失(cross_entropy)\n",
    "    # 1e-8是为了避免q_c全为0时造成tf.log无穷大\n",
    "    q_loss = tf.reduce_mean(-tf.reduce_sum(tf.log(q_c + 1e-8) * condition_label, 1))\n",
    "        \n",
    "    return g_loss, d_loss, q_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置优化器\n",
    "def get_optimizer(g_loss, d_loss, q_loss, beta1=0.4, learning_rate=0.001):\n",
    "    train_vars = tf.trainable_variables()\n",
    "    \n",
    "    g_vars = [var for var in train_vars if var.name.startswith(\"generator\")]\n",
    "    d_vars = [var for var in train_vars if var.name.startswith(\"discriminator\")]\n",
    "    q_vars = [var for var in train_vars if var.name.startswith(\"Q\")]\n",
    "    \n",
    "    # Optimizer\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "        # 这个需要注意的是Q网络优化的变量为g_vars + q_vars\n",
    "        q_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(q_loss, var_list=g_vars + q_vars)\n",
    "    \n",
    "    return g_opt, d_opt, q_opt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单独创建一个绘图函数用于后面绘制每一步生成器产生的图像\n",
    "def plot_image(samples):\n",
    "    # 由于生成器采用的激活函数是[-1, 1]之间，为了能够更好的绘图\n",
    "    # 需要将其转换到[0, 1]之间，因此有下面一行的操作\n",
    "    samples = (samples + 1) / 2\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=25, sharex=True, \n",
    "                             sharey=True, figsize=(50, 2))\n",
    "    for img, ax in zip(samples, axes):\n",
    "        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    fig.tight_layout(pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "batch_size = 64\n",
    "noise_size = 100\n",
    "epochs = 7\n",
    "n_samples = 25\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(noise_size, data_shape, batch_size, n_samples):\n",
    "   \n",
    "    # 存储loss\n",
    "    losses = []\n",
    "    steps = 0\n",
    "    \n",
    "    inputs_real, inputs_noise, condition_label = get_inputs(noise_size, data_shape[1], data_shape[2], data_shape[3])\n",
    "    g_loss, d_loss, q_loss = get_loss(inputs_real, inputs_noise, condition_label, data_shape[-1])\n",
    "    g_train_opt, d_train_opt, q_train_opt= get_optimizer(g_loss, d_loss, q_loss, beta1, learning_rate)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # 迭代epoch\n",
    "        for e in range(epochs):\n",
    "            for batch_i in range(mnist.train.num_examples//batch_size):\n",
    "                steps += 1\n",
    "                batch_images_, batch_labels = mnist.train.next_batch(batch_size)\n",
    "\n",
    "                batch_images = batch_images_.reshape((batch_size, data_shape[1], data_shape[2], data_shape[3]))\n",
    "                batch_images = batch_images*2 -1\n",
    "\n",
    "                # noise\n",
    "                batch_noise = np.random.uniform(-1, 1, size=(batch_size, noise_size))\n",
    "                c_labels = np.random.multinomial(1, 10*[0.1], size=batch_size)\n",
    "\n",
    "                # run optimizer\n",
    "                _ = sess.run(g_train_opt, feed_dict={inputs_real: batch_images,\n",
    "                                                     inputs_noise: batch_noise,\n",
    "                                                     condition_label: c_labels})\n",
    "                _ = sess.run(d_train_opt, feed_dict={inputs_real: batch_images,\n",
    "                                                     inputs_noise: batch_noise,\n",
    "                                                     condition_label: c_labels})\n",
    "                _ = sess.run(q_train_opt, feed_dict={inputs_real: batch_images,\n",
    "                                                     inputs_noise: batch_noise,\n",
    "                                                     condition_label: c_labels})\n",
    "                \n",
    "                if steps % 101 == 0:\n",
    "                    saver.save(sess, \"../tf_saver_files/class_7_of_infoGAN/generator.ckpt\")\n",
    "                    train_loss_d = d_loss.eval({inputs_real: batch_images,\n",
    "                                                inputs_noise: batch_noise,\n",
    "                                                condition_label:c_labels})\n",
    "                    train_loss_g = g_loss.eval({inputs_real: batch_images,\n",
    "                                                inputs_noise: batch_noise,\n",
    "                                                condition_label:c_labels})\n",
    "                    train_loss_q = q_loss.eval({inputs_real: batch_images,\n",
    "                                                inputs_noise: batch_noise,\n",
    "                                                condition_label:c_labels})\n",
    "                    \n",
    "                    losses.append((train_loss_d, train_loss_g, train_loss_q))\n",
    "                    # 显示图片\n",
    "                    c_labels = tf.to_float(c_labels)\n",
    "                    samples = sess.run(generator(batch_noise, data_shape[-1], c_labels, False))\n",
    "                    plot_images(samples)\n",
    "                    print(\"Epoch {}/{}....\".format(e+1, epochs), \n",
    "                          \"Discriminator Loss: {:.4f}....\".format(train_loss_d),\n",
    "                          \"Generator Loss: {:.4f}....\". format(train_loss_g))\n",
    "            saver.save(sess, \"../tf_saver_files/class_7_infoGAN/generator.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-6e404451fc98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-7418d9402a5e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(noise_size, data_shape, batch_size, n_samples)\u001b[0m\n\u001b[0;32m     34\u001b[0m                 _ = sess.run(q_train_opt, feed_dict={inputs_real: batch_images,\n\u001b[0;32m     35\u001b[0m                                                      \u001b[0minputs_noise\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_noise\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                                                      condition_label: c_labels})\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m101\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    train(noise_size, [-1, 28, 28, 1], batch_size, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape =[-1, 28, 28, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_real, inputs_noise, condition_label = get_inputs(noise_size, data_shape[1], data_shape[2], data_shape[3])\n",
    "g_loss, d_loss, q_loss = get_loss(inputs_real, inputs_noise, condition_label, data_shape[-1])\n",
    "g_train_opt, d_train_opt, q_train_opt= get_optimizer(g_loss, d_loss, q_loss, beta1, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "model_file=tf.train.latest_checkpoint('../tf_saver_files/class_7_of_infoGAN/generator.ckpt')\n",
    "saver.restore(sess, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "noise_size =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_noise = np.random.uniform(-1, 1, size=(batch_size, noise_size))\n",
    "c_labels = np.zeros((batch_size,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里制定我们的c_labels的第4个维度的值为1，下面可以看到在第4个维度置为1了之后，代表的时候控制数字2的生成\n",
    "# 这里需要注意的是condition_label每个维度都是准确的控制某个属性的生成的\n",
    "# 但是哪个位置对应哪个属性的生成并不是固定不变的，或者说不是一一对应的(比方说第2个维度控制数字2的生成就不是固定的，否则这里也不会要用到第四个维度才能控制数字2的生成)\n",
    "# 所以condition_label在训练完毕之后，每个人/每次训练完毕之后每个维度控制的属性都是不一样的，只是每个维度准备控制一个属性的生成这个是不变的，是指位置是不固定的或者说以一一对应的而已\n",
    "c_labels[:, 3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_labels = tf.cast(c_labels, tf.float32)\n",
    "batch_noise = tf.cast(batch_noise, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sess.run(generator(batch_noise, 1, c_labels, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
