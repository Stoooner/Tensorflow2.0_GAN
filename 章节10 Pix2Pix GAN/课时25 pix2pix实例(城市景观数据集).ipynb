{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课时25 pix2pixGAN实例(TF2.0城市街景数据集)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import glob\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1. **需要注意的是，原始的GAN的生成器网络接收的输入的一个一维的向量，因此整体的生成器网络其实只是一个AutoEncode的Decode部分；而pix2pixGAN的生成器网络部分接收的输入是一个完整的图片，因此pix2pixGAN的生成器网络是一个完整的AutoEncode网络。当我们使用pix2pixGAN网络进行类似图像翻译的任务的时候，输入与输出之间会共享很多的信息，例如图像轮廓信息等。而这个AutoEncode生成器网络在使用普通的卷积网络进行传递的信息传递的时候，每一层网络都要存储这些信息，会很容易出错，因此为了避免这样的情况发生，我们使用U-Net网络来搭建生成器网络。**\n",
    ">2. **pix2pixGAN本质上是一种图像翻译模型。也就是把一张图像翻译成另一张图像。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 定义数据集读取和预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = '../'\n",
    "train_images_path_list = glob.glob(pathname=train_images_path)\n",
    "len(train_images_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show(tf.keras.preprocessing.image.load_img(train_images_path_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义图片解码函数\n",
    "def read_jpg(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义归一化函数\n",
    "def normalize(mask, image):\n",
    "    mask = tf.cast(mask, tf.float32) / 127.5 - 1\n",
    "    image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "    return mask, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义图像加载函数\n",
    "def load_image(image_path):\n",
    "    image = read_jpg(image_path)\n",
    "    width = tf.shape(image)[1]\n",
    "    # 由于image和它对应的mask是合并到一张图片的，因此需要从中间切分开来\n",
    "    w = w // 2\n",
    "    input_mask = image[:, w:, :]\n",
    "    input_image = image[:, :w, :]\n",
    "    \n",
    "    # 下面两个resize的操作实际上没有什么实质性的影响，因为mask和image本身就是[256, 256, 3]的\n",
    "    # 但是下面这两个操作能够使得mask和image在做成datasets之后可以正常显示两个的shape大小\n",
    "    input_mask = tf.image.resize(input_mask, (256, 256))\n",
    "    input_image = tf.image.resize(input_image, (256, 256))\n",
    "    \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "    input_mask, input_image = normalize(input_mask, input_image)\n",
    "    return input_mask, input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = len(train_images_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建datasets\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_images_path_list)\n",
    "dataset = dataset.map(load_image)\n",
    "dataset = dataset.shuffer(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# GPU在训练当前批次的时候，使用prefetch函数能够让CPU去预加载另一批数据\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先展示一下图像（需要注意的是dataset.take(1)这里代表的是取一个batch的数据）\n",
    "for mask, img in dataset.take(1):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # 这里使用tf.keras.preprocessing.image.array_to_img是因为到这里mask和image已经归一化了，存在你显示不正常的情况\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(mask[0]))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(image[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面训练数据加载完毕之后开始加载测试数据\n",
    "test_images_path = '../'\n",
    "test_images_path_list = glob.glob(pathname=test_images_path)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(test_images_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义图像加载函数\n",
    "def load_image_test(image_path):\n",
    "    image = read_jpg(image_path)\n",
    "    width = tf.shape(image)[1]\n",
    "    # 由于image和它对应的mask是合并到一张图片的，因此需要从中间切分开来\n",
    "    w = w // 2\n",
    "    input_mask = image[:, w:, :]\n",
    "    input_image = image[:, :w, :]\n",
    "    \n",
    "    # 下面两个resize的操作实际上没有什么实质性的影响，因为mask和image本身就是[256, 256, 3]的\n",
    "    # 但是下面这两个操作能够使得mask和image在做成datasets之后可以正常显示两个的shape大小\n",
    "    input_mask = tf.image.resize(input_mask, (256, 256))\n",
    "    input_image = tf.image.resize(input_image, (256, 256))\n",
    "    \n",
    "    input_mask, input_image = normalize(input_mask, input_image)\n",
    "    return input_mask, input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dataset_test.map(load_image_test)\n",
    "dataset_test = dataset_test.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先展示一下图像（需要注意的是dataset.take(1)这里代表的是取一个batch的数据）\n",
    "for mask, img in dataset_test.take(1):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # 这里使用tf.keras.preprocessing.image.array_to_img是因为到这里mask和image已经归一化了，存在你显示不正常的情况\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(mask[0]))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(image[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义上采用和下采样模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sampling(filters, kernel_size, apply_bn_flag=True):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=2, \n",
    "                               padding='same', use_bias=False))\n",
    "    if apply_bn_flag:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_sampling(filters, kernel_size, apply_dropout_flag=False):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(filters=filters, kernel_size=kernel_size, \n",
    "                                              strides=2, padding='same', use_bias=False))\n",
    "\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    if apply_dropout_flag:\n",
    "        model.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Generator模型(U-Net架构)\n",
    "def Generator():\n",
    "    inputs = tf.keras.layers.Input(shape=(256, 256, 3))\n",
    "    down_stach = [\n",
    "        # [256, 256, 3] ===> [128, 128, 64]\n",
    "        down_sampling(filters=64, kernel_size=3, apply_bn_flag=False),\n",
    "        # [128, 128, 64] ===> [64, 64, 128]\n",
    "        down_sampling(filters=128, kernel_size=3),\n",
    "        # [64, 64, 128] ===> [32, 32, 256]\n",
    "        down_sampling(filters=256, kernel_size=3),\n",
    "        # [32, 32, 256] ===> [16, 16, 512]\n",
    "        down_sampling(filters=512, kernel_size=3),\n",
    "        \n",
    "        # [16, 16, 512] ===> [8, 8, 512]\n",
    "        down_sampling(filters=512, kernel_size=3),\n",
    "        # [8, 8, 512] ===> [4, 4, 512]\n",
    "        down_sampling(filters=512, kernel_size=3),\n",
    "        # [4, 4, 512] ===> [2, 2, 512]\n",
    "        down_sampling(filters=512, kernel_size=3),\n",
    "        # [2, 2, 512] ===> [1, 1, 512]\n",
    "        down_sampling(filters=512, kernel_size=3)\n",
    "    ]\n",
    "    \n",
    "    up_stach = [\n",
    "        # [1, 1, 512] ===> [2, 2, 512]\n",
    "        up_sampling(filters=512, kernel_size=3, apply_dropout_flag=True),\n",
    "        # [2, 2, 512] ===> [4, 4, 512]\n",
    "        up_sampling(filters=512, kernel_size=3, apply_dropout_flag=True),\n",
    "        # [4, 4, 512] ===> [8, 8, 512]\n",
    "        up_sampling(filters=512, kernel_size=3, apply_dropout_flag=True),\n",
    "        # [8, 8, 512] ===> [16, 16, 512]\n",
    "        up_sampling(filters=512, kernel_size=3),\n",
    "        \n",
    "        # [16, 16, 512] ===> [32, 32, 256]\n",
    "        up_sampling(filters=256, kernel_size=3),\n",
    "        # [32, 32, 256] ===> [64, 64, 128]\n",
    "        up_sampling(filters=128, kernel_size=3),\n",
    "        # [64, 64, 128] ===> [128, 128, 64]\n",
    "        up_sampling(filters=64, kernel_size=3)\n",
    "    ]\n",
    "    \n",
    "    x = inputs\n",
    "    skips = []\n",
    "    for down in down_stach:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "    skips = reversed(skips[:-1])\n",
    "    \n",
    "    for up, skip in zip(up_stach, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.concatenate([x, skip])\n",
    "        \n",
    "    # 由于图片的预处理中，对图片进行了[-1, 1]区间的归一化处理，因此使用activation='tanh'\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=3, strides=2, \n",
    "                                        padding='same', activation='tanh')(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(generator, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_31 (Sequential)      (None, 128, 128, 64) 1728        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_32 (Sequential)      (None, 64, 64, 128)  74240       sequential_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_33 (Sequential)      (None, 32, 32, 256)  295936      sequential_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_34 (Sequential)      (None, 16, 16, 512)  1181696     sequential_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_35 (Sequential)      (None, 8, 8, 512)    2361344     sequential_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_36 (Sequential)      (None, 4, 4, 512)    2361344     sequential_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_37 (Sequential)      (None, 2, 2, 512)    2361344     sequential_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_38 (Sequential)      (None, 1, 1, 512)    2361344     sequential_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_39 (Sequential)      (None, 2, 2, 512)    2361344     sequential_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2, 2, 1024)   0           sequential_39[0][0]              \n",
      "                                                                 sequential_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_40 (Sequential)      (None, 4, 4, 512)    4720640     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 4, 4, 1024)   0           sequential_40[0][0]              \n",
      "                                                                 sequential_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_41 (Sequential)      (None, 8, 8, 512)    4720640     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 8, 8, 1024)   0           sequential_41[0][0]              \n",
      "                                                                 sequential_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_42 (Sequential)      (None, 16, 16, 512)  4720640     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 16, 16, 1024) 0           sequential_42[0][0]              \n",
      "                                                                 sequential_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_43 (Sequential)      (None, 32, 32, 256)  2360320     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 512)  0           sequential_43[0][0]              \n",
      "                                                                 sequential_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_44 (Sequential)      (None, 64, 64, 128)  590336      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64, 64, 256)  0           sequential_44[0][0]              \n",
      "                                                                 sequential_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_45 (Sequential)      (None, 128, 128, 64) 147712      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128, 128, 128 0           sequential_45[0][0]              \n",
      "                                                                 sequential_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_21 (Conv2DTran (None, 256, 256, 3)  3459        concatenate_8[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 30,624,067\n",
      "Trainable params: 30,613,187\n",
      "Non-trainable params: 10,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    inputs = tf.keras.layers.Input(shape=(256, 256, 3))\n",
    "    targets = tf.keras.layers.Input(shape=(256, 256, 3))\n",
    "    \n",
    "    x = tf.keras.layers.concatenate([inputs, targets]) # [256, 256, 6]\n",
    "    x = down_sampling(filters=64, kernel_size=3, apply_bn_flag=False)(x) # [128, 128, 64]\n",
    "    x = down_sampling(filters=128, kernel_size=3)(x) # [64, 64, 128]\n",
    "    x = down_sampling(filters=256, kernel_size=3)(x) # [32, 32, 256]\n",
    "    x = tf.keras.layers.Conv2D(filters=512, kernel_size=3, strides=1, \n",
    "                               padding='same', use_bias=False)(x) # [32, 32, 512]\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters=1, kernel_size=3, strides=1)(x) # [30, 30, 512]\n",
    "    return tf.keras.Model(inputs=[inputs, targets], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 256, 256, 6)  0           input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_49 (Sequential)      (None, 128, 128, 64) 3456        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_50 (Sequential)      (None, 64, 64, 128)  74240       sequential_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_51 (Sequential)      (None, 32, 32, 256)  295936      sequential_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 32, 512)  1179648     sequential_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 32, 32, 512)  2048        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, 32, 32, 512)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 30, 30, 1)    4609        leaky_re_lu_31[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,559,937\n",
      "Trainable params: 1,558,145\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(d_gen_output, gen_output, target):\n",
    "    gen_loss = loss_func(y_true=tf.ones_like(d_gen_output), y_pred=d_gen_output)\n",
    "    l1_loss = tf.reduce_mean(tf.abs(gen_output-target))\n",
    "    return gen_loss + l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(d_real_output, d_fake_output):\n",
    "    real_loss = loss_func(y_true=tf.ones_like(d_real_output), y_pred=d_real_output)\n",
    "    fake_loss = loss_func(y_true=tf.zeros_like(d_fake_output), y_pred=d_fake_output)\n",
    "    \n",
    "    return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_inputs, tar):\n",
    "    predition = model(test_inputs, training=True)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    display_list = [test_inputs[0], tar[0], predition[0]]\n",
    "    titles = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "    \n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(titles[i])\n",
    "        plt.imshow(display_list[i]*0.5+0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
