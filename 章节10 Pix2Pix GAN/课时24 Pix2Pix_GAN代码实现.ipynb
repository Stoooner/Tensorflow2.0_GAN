{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课时24 Pix2Pix_GAN代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import glob\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**需要注意的是，原始的GAN的生成器网络接收的输入的一个一维的向量，因此整体的生成器网络其实只是一个AutoEncode的Decode部分；而pix2pixGAN的生成器网络部分接收的输入是一个完整的图片，因此pix2pixGAN的生成器网络是一个完整的AutoEncode网络。当我们使用pix2pixGAN网络进行类似图像翻译的任务的时候，输入与输出之间会共享很多的信息，例如图像轮廓信息等。而这个AutoEncode生成器网络在使用普通的卷积网络进行传递的信息传递的时候，每一层网络都要存储这些信息，会很容易出错，因此为了避免这样的情况发生，我们使用U-Net网络来搭建生成器网络。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 定义Generator模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成器采用的是U-Net网络结果，U-Net采用的也是Encode-Decode的结构\n",
    "# 其中Encode是卷积结构，Decode是反卷积结构\n",
    "def generator(inputs_real, is_train=True, alpha=0.01):\n",
    "    # [256, 256, 3]\n",
    "    with tf.variable_scope(name_or_scope='generator', reuse=(not is_train)):\n",
    "        # Encode网络部分：\n",
    "        # [128, 128, 64]\n",
    "        conv_1 = tf.layers.conv2d(inputs=inputs_real, filters=64, kernel_size=(3, 3), padding='same')\n",
    "        conv_1 = tf.nn.relu(conv_1)\n",
    "        conv_1 = tf.layers.max_pooling2d(inputs=conv_1, pool_size=(2, 2), strides=(2, 2), padding='same')\n",
    "        \n",
    "        # [64, 64, 128]\n",
    "        conv_2 = tf.layers.conv2d(inputs=conv_1, filters=128, kernel_size=(3, 3), padding='same')\n",
    "        conv_2 = tf.nn.relu(conv_2)\n",
    "        conv_2 = tf.layers.max_pooling2d(inputs=conv_2, pool_size=(2, 2), strides=(2, 2), padding='same')\n",
    "        \n",
    "        # [32, 32, 256]\n",
    "        conv_3 = tf.layers.conv2d(inputs=conv_2, filters=256, kernel_size=(3, 3), padding='same')\n",
    "        conv_3 = tf.nn.relu(conv_3)\n",
    "        conv_3 = tf.layers.max_pooling2d(inputs=conv_3, pool_size=(2, 2), strides=(2, 2), padding='same')\n",
    "        \n",
    "        # [16, 16, 512]\n",
    "        conv_4 = tf.layers.conv2d(inputs=conv_3, filters=512, kernel_size=(3, 3), padding='same')\n",
    "        conv_4 = tf.nn.relu(conv_4)\n",
    "        conv_4 = tf.layers.max_pooling2d(inputs=conv_4, pool_size=(2, 2), strides=(2, 2), padding='same')\n",
    "        \n",
    "        # [8, 8, 512]\n",
    "        conv_5 = tf.layers.conv2d(inputs=conv_4, filters=512, kernel_size=(3, 3), padding='same')\n",
    "        conv_5 = tf.nn.relu(conv_5)\n",
    "        conv_5 = tf.layers.max_pooling2d(inputs=conv_5, pool_size=(2, 2), strides=(2, 2), padding='same')\n",
    "        \n",
    "        # [4, 4, 512]\n",
    "        conv_6 = tf.layers.conv2d(inputs=conv_5, filters=512, kernel_size=(3, 3), padding='same')\n",
    "        conv_6 = tf.nn.relu(conv_6)\n",
    "        conv_6 = tf.layers.max_pooling2d(inputs=conv_6, pool_size=(2, 2), strides=(2, 2), padding='same')\n",
    "        \n",
    "        # [2, 2, 512]\n",
    "        conv_7 = tf.layers.conv2d(inputs=conv_6, filters=512, kernel_size=(3, 3), padding='same')\n",
    "        conv_7 = tf.nn.relu(conv_7)\n",
    "        conv_7 = tf.layers.max_pooling2d(inputs=conv_7, pool_size=(2, 2), strides=(2, 2), padding='same')\n",
    "        \n",
    "        # [1, 1, 512]\n",
    "        conv_8 = tf.layers.conv2d(inputs=conv_7, filters=512, kernel_size=(3, 3), padding='same')\n",
    "        conv_8 = tf.nn.relu(conv_8)\n",
    "        conv_8 = tf.layers.max_pooling2d(inputs=conv_8, pool_size=(2, 2), strides=(2, 2), padding='same')\n",
    "        \n",
    "        # Dncode网络部分：\n",
    "        # [2, 2, 512]\n",
    "        conv_9 = tf.layers.conv2d_transpose(inputs=conv_8, filters=512, kernel_size=3, strides=2, padding='same')\n",
    "        conv_9 = tf.layers.batch_normalization(conv_9, training=is_train)\n",
    "        conv_9 = tf.nn.relu(conv_9)\n",
    "        conv_9 = tf.nn.dropout(conv_9, keep_prob=0.5)\n",
    "        \n",
    "        # [4, 4, 512]\n",
    "        conv_10 = tf.concat([conv_9, conv_7], axis=3)\n",
    "        conv_10 = tf.layers.conv2d_transpose(inputs=conv_10, filters=512, kernel_size=3, strides=2, padding='same')\n",
    "        conv_10 = tf.layers.batch_normalization(conv_10, training=is_train)\n",
    "        conv_10 = tf.nn.relu(conv_10)\n",
    "        conv_10 = tf.nn.dropout(conv_10, keep_prob=0.5)\n",
    "        \n",
    "        # [8, 8, 512]\n",
    "        conv_11 = tf.concat([conv_10, conv_6], axis=3)\n",
    "        conv_11 = tf.layers.conv2d_transpose(inputs=conv_11, filters=512, kernel_size=3, strides=2, padding='same')\n",
    "        conv_11 = tf.layers.batch_normalization(conv_11, training=is_train)\n",
    "        conv_11 = tf.nn.relu(conv_11)\n",
    "        conv_11 = tf.nn.dropout(conv_11, keep_prob=0.5)\n",
    "        \n",
    "        # [16, 16, 512]\n",
    "        conv_12 = tf.concat([conv_11, conv_5], axis=3)\n",
    "        conv_12 = tf.layers.conv2d_transpose(inputs=conv_12, filters=512, kernel_size=3, strides=2, padding='same')\n",
    "        conv_12 = tf.layers.batch_normalization(conv_12, training=is_train)\n",
    "        conv_12 = tf.nn.relu(conv_12)\n",
    "        \n",
    "        # [32, 32, 256]\n",
    "        conv_13 = tf.concat([conv_12, conv_4], axis=3)\n",
    "        conv_13 = tf.layers.conv2d_transpose(inputs=conv_13, filters=256, kernel_size=3, strides=2, padding='same')\n",
    "        conv_13 = tf.layers.batch_normalization(conv_13, training=is_train)\n",
    "        conv_13 = tf.nn.relu(conv_13)\n",
    "        \n",
    "        # [64, 64, 128]\n",
    "        conv_14 = tf.concat([conv_13, conv_3], axis=3)\n",
    "        conv_14 = tf.layers.conv2d_transpose(inputs=conv_14, filters=128, kernel_size=3, strides=2, padding='same')\n",
    "        conv_14 = tf.layers.batch_normalization(conv_14, training=is_train)\n",
    "        conv_14 = tf.nn.relu(conv_14)\n",
    "        \n",
    "        # [128, 128, 64]\n",
    "        conv_15 = tf.concat([conv_14, conv_2], axis=3)\n",
    "        conv_15 = tf.layers.conv2d_transpose(inputs=conv_15, filters=64, kernel_size=3, strides=2, padding='same')\n",
    "        conv_15 = tf.layers.batch_normalization(conv_15, training=is_train)\n",
    "        conv_15 = tf.nn.relu(conv_15)\n",
    "        \n",
    "        # [256, 256, 3]\n",
    "        conv_16 = tf.concat([conv_15, conv_1], axis=3)\n",
    "        conv_16 = tf.layers.conv2d_transpose(inputs=conv_16, filters=3, kernel_size=3, strides=2, padding='same')\n",
    "        conv_16 = tf.layers.batch_normalization(conv_16, training=is_train)\n",
    "        \n",
    "        # 图像归一化\n",
    "        outputs = tf.nn.tanh(conv_16)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义Discriminator模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要注意的是Discriminator接收成对的输入\n",
    "def discriminator(inputs_real, inputs_cartoon, reuse=False, alpha=0.01):\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        # 由于要同时对输入的图像和卡通图像进行判别，得出一个结果(1/0)，所以将这两个输入的图像进行合并\n",
    "        layer0 = tf.concat([inputs_real, inputs_cartoon], axis=3)\n",
    "\n",
    "        layer1 = tf.layers.conv2d(layer0, 64, 3, strides=2, padding='same')\n",
    "        layer1 = tf.maximum(alpha*layer1, layer1)\n",
    "        \n",
    "        layer2 = tf.layers.conv2d(layer1, 128, 3, strides=2, padding='same')\n",
    "        layer2 = tf.layers.batch_normalization(layer2, training=True)\n",
    "        layer2 = tf.maximum(alpha*layer2, layer2)\n",
    "        \n",
    "        layer3 = tf.layers.conv2d(layer2, 256, 3, strides=2, padding='same')\n",
    "        layer3 = tf.layers.batch_normalization(layer3, training=True)\n",
    "        layer3 = tf.maximum(alpha*layer3, layer3)\n",
    "        \n",
    "        layer4 = tf.layers.conv2d(layer3, 512, 3, strides=2, padding='same')\n",
    "        layer4 = tf.layers.batch_normalization(layer4, training=True)\n",
    "        layer4 = tf.maximum(alpha*layer4, layer4)\n",
    "        \n",
    "        flatten = tf.reshape(layer4, (-1, 16*16*512))\n",
    "        logits = tf.layers.dense(flatten, 1)\n",
    "        # 通过sigmoid判断输入的类别(1/0)\n",
    "        outputs = tf.sigmoid(logits)\n",
    "        \n",
    "        return logits, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义Loss模块\n",
    "- D网络损失函数：\n",
    ">- 1. 输入真实的成对图像希望判定为1；\n",
    ">- 2. 输入生成图像和原图像希望判定位0；\n",
    "- G网络损失函数：\n",
    ">- 1. 输入生成图像和原图像希望判别为1；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(inputs_image, inputs_cartoons, smooth=0.01):\n",
    "    g_outputs = generator(inputs_image, is_train=True)\n",
    "    d_logits_real, d_outputs_real = discriminator(inputs_image, inputs_cartoons)\n",
    "    d_logits_fake, d_outputs_fake = discriminator(inputs_image, g_outputs, reuse=True)\n",
    "    \n",
    "    # 计算d_loss\n",
    "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n",
    "                                                                         labels=tf.ones_like(d_outputs_real)*(1-smooth)))\n",
    "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                                         labels=tf.zeros_like(d_outputs_fake)))\n",
    "    \n",
    "    # 计算g_loss\n",
    "    g_loss_gan = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                                        labels=tf.ones_like(d_outputs_fake)*(1-smooth)))\n",
    "    # 为了约束生成网络，将生成网络生成好了的图像与原始的输入生成网络的图像的标签进行L1约束\n",
    "    # 将生成了的图像扁平化成一个一维向量，方便L1约束计算\n",
    "    g_outputs_logits = tf.reshape(g_outputs, [-1, 256*256*3])\n",
    "    # 输入生成网络的图像原始的对应标签也扁平化成一维向量，方便L1约束计算\n",
    "    inputs_cartoons_logits = tf.reshape(inputs_cartoons, [-1, 256*256*3])\n",
    "    g_loss_L1 = tf.reduce_mean(tf.reduce_sum(tf.abs(g_outputs_logits-inputs_cartoons_logits)))\n",
    "    \n",
    "    # 计算Loss和\n",
    "    g_loss = tf.add(g_loss_gan, g_loss_L1)\n",
    "    d_loss = tf.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 定义optimizer模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(g_loss, d_loss, beta1=0.4, learning_rate=0.001):\n",
    "    train_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in train_vars if var.name.startswith(\"generator\")]\n",
    "    d_vars = [var for var in train_vars if var.name.startswith(\"discriminator\")]\n",
    "    \n",
    "    # Optimizer\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "    \n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 定义辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(samples):\n",
    "    # 由于生成模型生成的数据是使用tanh输出的，所以其值处于[-1, 1]之间，不适合画图\n",
    "    # 因此这里先需要将图像的值转换到[0, 1]之间才行\n",
    "    samples = (samples + 1) / 2\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(10,2))\n",
    "    for img, ax in zip(samples, axes):\n",
    "        # 然后这里需要注意的是这里需将其reshape成原始图像对应的大小尺寸\n",
    "        ax.imshow(img.reshape((250, 200, 3)))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    fig.tight_layout(pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_generator_output(sess, samp_images):\n",
    "    samples = sess.run(generator(samp_images, False))\n",
    "    samples = sess.run(tf.image.resize_image_with_crop_or_pad(samples, 250, 200))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 定义训练模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # 存储loss\n",
    "    losses = []\n",
    "    steps = 300\n",
    "        \n",
    "    image_filenames = glob.glob('../日月光华-GAN大型数据集/1.0部分代码和数据集/数据集/pix2pixGAN数据集/training_photos/*.jpg')\n",
    "    cartoon_filenames = glob.glob('日月光华-GAN大型数据集/1.0部分代码和数据集/数据集/pix2pixGAN数据集/training__sketches/*.jpg')\n",
    "    \n",
    "    image_que = tf.train.slice_input_producer([image_filenames, cartoon_filenames], \n",
    "                                              shuffle=True)\n",
    "    image_ = tf.read_file(image_que[0])\n",
    "    image = tf.image.decode_jpeg(image_, channels=3)\n",
    "    image = tf.image.resize_image_with_crop_or_pad(image, 256, 256)\n",
    "    new_img = tf.image.per_image_standardization(image)\n",
    "        \n",
    "    cartoon_ = tf.read_file(image_que[1])\n",
    "    cartoon = tf.image.decode_jpeg(cartoon_, channels=3)\n",
    "    cartoon = tf.image.resize_image_with_crop_or_pad(cartoon, 256, 256)\n",
    "    new_cartoon = tf.image.per_image_standardization(cartoon)\n",
    "    \n",
    "    batch_size = 5\n",
    "    capacity = 3 + 2*batch_size\n",
    "          \n",
    "    image_batch, cartoon_batch = tf.train.batch([new_img, new_cartoon], batch_size=batch_size, capacity=capacity)\n",
    "    \n",
    "    g_loss, d_loss = get_loss(image_batch, cartoon_batch)\n",
    "    g_train_opt, d_train_opt = get_optimizer(g_loss, d_loss, beta1, learning_rate)\n",
    "    \n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # 迭代epoch\n",
    "        for e in range(steps):\n",
    "            # run optimizer\n",
    "            _ = sess.run(g_train_opt)\n",
    "            _ = sess.run(d_train_opt)\n",
    "                \n",
    "            if e % 50 == 0:\n",
    "                saver.save(sess,'../tf_saver_files/class_10_of_pix2pix/generator.ckpt',global_step=e)\n",
    "                train_loss_d = d_loss.eval()\n",
    "                \n",
    "                train_loss_g = g_loss.eval()\n",
    "                losses.append((train_loss_d, train_loss_g))\n",
    "                # 显示图片\n",
    "                samples = show_generator_output(sess, image_batch)\n",
    "                plot_images(samples)\n",
    "                print(\"Epoch {}/{}....\".format(e+1, steps), \n",
    "                      \"Discriminator Loss: {:.4f}....\".format(train_loss_d),\n",
    "                      \"Generator Loss: {:.4f}....\". format(train_loss_g))\n",
    "        saver.save(sess,'../tf_saver_files/class_10_of_pix2pix/generator.ckpt',global_step=steps)\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'filename' of 'ReadFile' Op has type float32 that does not match expected type of string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    510\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    512\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\u001b[0m\n\u001b[0;32m   1174\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[1;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[1;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[0;32m    978\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor conversion requested dtype string for Tensor with dtype float32: 'Tensor(\"input_producer/GatherV2_1:0\", shape=(), dtype=float32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-3fef00bbdc15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-b4d60c6b190c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mnew_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mper_image_standardization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mcartoon_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_que\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mcartoon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_jpeg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcartoon_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mcartoon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_image_with_crop_or_pad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcartoon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(filename, name)\u001b[0m\n\u001b[0;32m    611\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m--> 613\u001b[1;33m         \"ReadFile\", filename=filename, name=name)\n\u001b[0m\u001b[0;32m    614\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m     result = _dispatch.dispatch(\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtypes_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDT_INVALID\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m               raise TypeError(\"%s expected type of %s.\" %\n\u001b[1;32m--> 534\u001b[1;33m                               (prefix, dtypes.as_dtype(input_arg.type).name))\n\u001b[0m\u001b[0;32m    535\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m               \u001b[1;31m# Update the maps with the default, if needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Input 'filename' of 'ReadFile' Op has type float32 that does not match expected type of string."
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
