{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 课时28 Cycle_GAN代码实现(苹果转橘子)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 定义Generator模块(CycleGAN中的Generator也是U-Net网络)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(inputs_real, is_train=True, alpha=0.01, name=\"generator\"):\n",
    "    # 由于Cycle_GAN中有两个Generator网络(G_AB和G_BA)，为了代码复用，所以定义了name这个参数\n",
    "    # 通过name不同的赋值来区分不同的生成器Generator，以达到代码复用的目的\n",
    "    # 256*256*3\n",
    "    with tf.variable_scope(name, reuse=(not is_train)):\n",
    "        # 128*128*64\n",
    "        conv1 = tf.layers.conv2d(inputs_real, 64, (3,3), padding='same')\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, (2,2), (2,2), padding='same')\n",
    "        # 64*64*128\n",
    "        conv2 = tf.layers.conv2d(conv1, 128, (3,3), padding='same')\n",
    "        conv2 = tf.nn.relu(conv2)\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same')\n",
    "        # 32*32*256\n",
    "        conv3 = tf.layers.conv2d(conv2, 256, (3,3), padding='same')\n",
    "        conv3 = tf.nn.relu(conv3)\n",
    "        conv3 = tf.layers.max_pooling2d(conv3, (2,2), (2,2), padding='same')\n",
    "        # 16*16*512\n",
    "        conv4 = tf.layers.conv2d(conv3, 512, (3,3), padding='same')\n",
    "        conv4 = tf.nn.relu(conv4)\n",
    "        conv4 = tf.layers.max_pooling2d(conv4, (2,2), (2,2), padding='same')\n",
    "        # 8*8*512\n",
    "        conv5 = tf.layers.conv2d(conv4, 512, (3,3), padding='same')\n",
    "        conv5 = tf.nn.relu(conv5)\n",
    "        conv5 = tf.layers.max_pooling2d(conv5, (2,2), (2,2), padding='same')\n",
    "        # 4*4*512\n",
    "        conv6 = tf.layers.conv2d(conv5, 512, (3,3), padding='same')\n",
    "        conv6 = tf.nn.relu(conv6)\n",
    "        conv6 = tf.layers.max_pooling2d(conv6, (2,2), (2,2), padding='same')\n",
    "        # 2*2*512\n",
    "        conv7 = tf.layers.conv2d(conv6, 512, (3,3), padding='same')\n",
    "        conv7 = tf.nn.relu(conv7)\n",
    "        conv7 = tf.layers.max_pooling2d(conv7, (2,2), (2,2), padding='same')\n",
    "        # 1*1*512\n",
    "        conv8 = tf.layers.conv2d(conv7, 512, (3,3), padding='same')\n",
    "        conv8 = tf.nn.relu(conv8)\n",
    "        conv8 = tf.layers.max_pooling2d(conv8, (2,2), (2,2), padding='same')\n",
    "        \n",
    "        \n",
    "        # 2*2*512\n",
    "        conv9 = tf.layers.conv2d_transpose(conv8, 512, 3, strides=2, padding='same')\n",
    "        conv9 = tf.layers.batch_normalization(conv9, training=is_train)\n",
    "        conv9 = tf.nn.relu(conv9)\n",
    "        conv9 = tf.nn.dropout(conv9, keep_prob=0.5)\n",
    "        # 4*4*512\n",
    "        conv10 = tf.concat([conv9,conv7], 3)\n",
    "        conv10 = tf.layers.conv2d_transpose(conv10, 512, 3, strides=2, padding='same')\n",
    "        conv10 = tf.layers.batch_normalization(conv10, training=is_train)\n",
    "        conv10 = tf.nn.relu(conv10)\n",
    "        conv10 = tf.nn.dropout(conv10, keep_prob=0.5)\n",
    "        # 8*8*512\n",
    "        conv11 = tf.concat([conv10,conv6], 3)\n",
    "        conv11 = tf.layers.conv2d_transpose(conv11, 512, 3, strides=2, padding='same')\n",
    "        conv11 = tf.layers.batch_normalization(conv11, training=is_train)\n",
    "        conv11 = tf.nn.relu(conv11)\n",
    "        conv11 = tf.nn.dropout(conv11, keep_prob=0.5)\n",
    "        # 16*16*512\n",
    "        conv12 = tf.concat([conv11,conv5], 3)\n",
    "        conv12 = tf.layers.conv2d_transpose(conv12, 512, 3, strides=2, padding='same')\n",
    "        conv12 = tf.layers.batch_normalization(conv12, training=is_train)\n",
    "        conv12 = tf.nn.relu(conv12)\n",
    "        # 32*32*256\n",
    "        conv13 = tf.concat([conv12,conv4], 3)\n",
    "        conv13 = tf.layers.conv2d_transpose(conv13, 256, 3, strides=2, padding='same')\n",
    "        conv13 = tf.layers.batch_normalization(conv13, training=is_train)\n",
    "        conv13 = tf.nn.relu(conv13)\n",
    "        # 64*64*128\n",
    "        conv14 = tf.concat([conv13,conv3], 3)\n",
    "        conv14 = tf.layers.conv2d_transpose(conv14, 128, 3, strides=2, padding='same')\n",
    "        conv14 = tf.layers.batch_normalization(conv14, training=is_train)\n",
    "        conv14 = tf.nn.relu(conv14)\n",
    "        # 128*128*64\n",
    "        conv15 = tf.concat([conv14,conv2], 3)\n",
    "        conv15 = tf.layers.conv2d_transpose(conv15, 64, 3, strides=2, padding='same')\n",
    "        conv15 = tf.layers.batch_normalization(conv15, training=is_train)\n",
    "        conv15 = tf.nn.relu(conv15)\n",
    "        # 256*256*3\n",
    "        conv16 = tf.concat([conv15,conv1], 3)\n",
    "        conv16 = tf.layers.conv2d_transpose(conv16, 3, 3, strides=2, padding='same')\n",
    "        # conv16 = tf.layers.batch_normalization(conv16, training=is_train)\n",
    "    \n",
    "        # 图片归一化(将图像的值转换到[-1, 1]区间)\n",
    "        outputs = tf.nn.tanh(conv16)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义Discriminator模块(CycleGAN中的Discriminator也有两个网络)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs_image, reuse=False, alpha=0.01, name=\"discriminator\"):\n",
    "    # 由于Discriminator也是两个网络，所以为了能够复用代码，也是通过name来进行判别器区分的\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        layer1 = tf.layers.conv2d(inputs_image, 64, 3, strides=2, padding='same')\n",
    "        layer1 = tf.layers.batch_normalization(layer1, training=True)\n",
    "        layer1 = tf.maximum(alpha * layer1, layer1)\n",
    "        \n",
    "        layer2 = tf.layers.conv2d(layer1, 64, 3, strides=2, padding='same')\n",
    "        layer2 = tf.layers.batch_normalization(layer2, training=True)\n",
    "        layer2 = tf.maximum(alpha * layer2, layer2)\n",
    "        \n",
    "        layer3 = tf.layers.conv2d(layer2, 128, 3, strides=2, padding='same')\n",
    "        layer3 = tf.layers.batch_normalization(layer3, training=True)\n",
    "        layer3 = tf.maximum(alpha * layer3, layer3)\n",
    "        \n",
    "        layer4 = tf.layers.conv2d(layer3, 128, 3, strides=2, padding='same')\n",
    "        layer4 = tf.layers.batch_normalization(layer4, training=True)\n",
    "        layer4 = tf.maximum(alpha * layer4, layer4)\n",
    "        \n",
    "        layer5 = tf.layers.conv2d(layer4, 256, 3, strides=2, padding='same')\n",
    "        layer5 = tf.layers.batch_normalization(layer5, training=True)\n",
    "        layer5 = tf.maximum(alpha * layer5, layer5)\n",
    "        \n",
    "        layer6 = tf.layers.conv2d(layer5, 256, 3, strides=2, padding='same')\n",
    "        layer6 = tf.layers.batch_normalization(layer6, training=True)\n",
    "        layer6 = tf.maximum(alpha * layer6, layer6)\n",
    "        \n",
    "        layer7 = tf.layers.conv2d(layer6, 512, 3, strides=2, padding='same')\n",
    "        layer7 = tf.layers.batch_normalization(layer7, training=True)\n",
    "        layer7 = tf.maximum(alpha * layer7, layer7)\n",
    "        \n",
    "        flatten = tf.reshape(layer7, (-1, 2*2*512))\n",
    "        logits = tf.layers.dense(flatten, 1)\n",
    "        outputs = tf.sigmoid(logits)\n",
    "        \n",
    "        return  logits, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义Loss损失函数"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_loss(inputs_images, inputs_cartoons, smooth=0.1):\n",
    "    fake_cartoons = generator(inputs_images, name=\"generatorI2C\")\n",
    "    fake_images_ = generator(fake_cartoons, name=\"generatorC2I\")\n",
    "    fake_images = generator(inputs_cartoons, False, name=\"generatorC2I\")\n",
    "    fake_cartoons_ = generator(fake_images, False, name=\"generatorI2C\")\n",
    "\n",
    "    discriminator_cartoon_fake, cartoon_fake_logits = discriminator(fake_cartoons, reuse=False, name=\"discriminator_cartoon\")\n",
    "    discriminator_image_fake, image_fake_logits = discriminator(fake_images, reuse=False, name=\"discriminator_image\")\n",
    "    \n",
    "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=cartoon_fake_logits, \n",
    "                                                                    labels=tf.ones_like(discriminator_cartoon_fake)*(1-smooth))) \\\n",
    "            + tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=image_fake_logits,\n",
    "                                                                    labels=tf.ones_like(discriminator_image_fake)*(1-smooth))) \\\n",
    "            + tf.reduce_mean(tf.abs(inputs_images - fake_images_))\\\n",
    "            + tf.reduce_mean(tf.abs(inputs_cartoons - fake_cartoons_))\n",
    "            \n",
    "            \n",
    "    \n",
    "    discriminator_cartoon_real, cartoon_real_logits = discriminator(inputs_cartoons, reuse=True, name=\"discriminator_cartoon\")\n",
    "    discriminator_image_real, image_real_logits = discriminator(inputs_images, reuse=True, name=\"discriminator_image\")\n",
    "    \n",
    "    d_cartoon_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=cartoon_real_logits, \n",
    "                                                                                 labels=tf.ones_like(discriminator_cartoon_real)*(1-smooth)))\n",
    "    d_image_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=image_real_logits, \n",
    "                                                                                 labels=tf.ones_like(discriminator_image_real)*(1-smooth)))\n",
    "    d_cartoon_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=cartoon_fake_logits, \n",
    "                                                                                 labels=tf.zeros_like(discriminator_cartoon_fake)*(1-smooth)))\n",
    "    d_image_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=image_fake_logits, \n",
    "                                                                                 labels=tf.zeros_like(discriminator_image_fake)*(1-smooth)))\n",
    "    \n",
    "    d_loss = d_cartoon_real_loss + d_image_real_loss + d_cartoon_fake_loss + d_image_fake_loss\n",
    "\n",
    "    \n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle_GAN中有四个loss\n",
    "def get_loss(inputs_images, inputs_cartoons, smooth=0.1):\n",
    "    \"\"\"\n",
    "    inputs_images: 实际指的就是inputs_apple\n",
    "    inputs_cartoons: 实际指的就是inputs_oringe\n",
    "    \"\"\"\n",
    "    # 输入inputs_images(真实的苹果图片) ===> 生成fake_cartoons(生成的橘子图片)\n",
    "    fake_cartoons = generator(inputs_images, name=\"generatorI2C\")\n",
    "    # 输入fake_cartoons(生成的橘子图片) ===> 生成fake_images_(再生成的苹果图片)\n",
    "    fake_images_ = generator(fake_cartoons, name=\"generatorC2I\")\n",
    "    # ====================================================================\n",
    "    fake_images = generator(inputs_cartoons, False, name=\"generatorC2I\")\n",
    "    fake_cartoons_ = generator(fake_images, False, name=\"generatorI2C\")\n",
    "    \n",
    "    # 这个判别器判断输入的图片是真实的橘子还是生成的橘子\n",
    "    discriminator_cartoon_fake, cartoon_fake_logits = discriminator(fake_cartoons, \n",
    "                                                                    reuse=False, \n",
    "                                                                    name=\"discriminator_cartoon\")\n",
    "    discriminator_cartoon_real, cartoon_real_logits = discriminator(inputs_cartoons, \n",
    "                                                                    reuse=True, \n",
    "                                                                    name=\"discriminator_cartoon\")\n",
    "    # 这个判别器判断输入的图片是否是真实的橘子或者苹果\n",
    "    discriminator_image_fake, image_fake_logits = discriminator(fake_images, \n",
    "                                                                reuse=False, \n",
    "                                                                name=\"discriminator_image\")\n",
    "    discriminator_image_real, image_real_logits = discriminator(inputs_images, \n",
    "                                                                reuse=True, \n",
    "                                                                name=\"discriminator_image\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 定义generator的损失loss\n",
    "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=cartoon_fake_logits, \n",
    "                                                                    labels=tf.ones_like(discriminator_cartoon_fake)*(1-smooth))) \\\n",
    "            + tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=image_fake_logits,\n",
    "                                                                    labels=tf.ones_like(discriminator_image_fake)*(1-smooth))) \\\n",
    "            + tf.reduce_mean(tf.abs(inputs_images - fake_images_))\\\n",
    "            + tf.reduce_mean(tf.abs(inputs_cartoons - fake_cartoons_))\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    d_cartoon_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=cartoon_real_logits, \n",
    "                                                                                 labels=tf.ones_like(discriminator_cartoon_real)*(1-smooth)))\n",
    "    d_cartoon_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=cartoon_fake_logits, \n",
    "                                                                                 labels=tf.zeros_like(discriminator_cartoon_fake)))\n",
    "    # =============================================================================================\n",
    "    d_image_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=image_real_logits, \n",
    "                                                                               labels=tf.ones_like(discriminator_image_real)*(1-smooth)))\n",
    "    d_image_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=image_fake_logits, \n",
    "                                                                               labels=tf.zeros_like(discriminator_image_fake)))\n",
    "    \n",
    "    d_loss = d_cartoon_real_loss + d_image_real_loss + d_cartoon_fake_loss + d_image_fake_loss\n",
    "\n",
    "    \n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 定义优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(g_loss, d_loss, g_loss_tran, learning_rate=0.0001):\n",
    "    \n",
    "    train_vars = tf.trainable_variables()\n",
    "    \n",
    "    g_vars = [var for var in train_vars if var.name.startswith(\"generator\")]\n",
    "    d_vars = [var for var in train_vars if var.name.startswith(\"discriminator\")]\n",
    "    \n",
    "    \n",
    "    # Optimizer\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate*5).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "    \n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 定义辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(samples):\n",
    "    samples = (samples + 1) / 2\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(10,1))\n",
    "    for img, ax in zip(samples, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    fig.tight_layout(pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_generator_output(sess, samp_images):\n",
    "    samples = sess.run(generator(samp_images, False , name=\"generatorI2C\"))\n",
    "    samples = sess.run(tf.reshape(samples, [-1, 256, 256, 3]))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "beta1 = 0.4\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    # 存储loss\n",
    "    losses = []\n",
    "    steps = 1000\n",
    "    \n",
    "    apple_list = glob.glob('../日月光华-GAN大型数据集/1.0部分代码和数据集/数据集/apple2orange/trainA/*.jpg')\n",
    "    orange_list = glob.glob('../日月光华-GAN大型数据集/1.0部分代码和数据集/数据集/apple2orange/trainB/*.jpg')\n",
    "    image_que = tf.train.slice_input_producer([apple_list, orange_list], shuffle=True)\n",
    "    \n",
    "    image_ = tf.read_file(image_que[0])\n",
    "    image = tf.image.decode_jpeg(image_, channels=3)\n",
    "    image = tf.image.resize_image_with_crop_or_pad(image, 256, 256)\n",
    "    new_img = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    new_img = new_img*2 -1\n",
    "    \n",
    "    cartoon_ = tf.read_file(image_que[1])\n",
    "    cartoon = tf.image.decode_jpeg(cartoon_, channels=3)\n",
    "    cartoon = tf.image.resize_image_with_crop_or_pad(cartoon, 256, 256)\n",
    "    # tf.image.convert_image_dtype除了将数据转换为tf.float32之外，还会将数据转换到[0, 1]之间\n",
    "    new_cartoon = tf.image.convert_image_dtype(cartoon, tf.float32)\n",
    "    # 为了后续能够跟生成数据进行对比，需要将数据转换到[-1, 1]之间\n",
    "    new_cartoon = new_cartoon*2 -1\n",
    "    \n",
    "    batch_size = 10\n",
    "    capacity = 3 + 2 * batch_size\n",
    "          \n",
    "    image_batch, cartoon_batch = tf.train.batch([new_img, new_cartoon], batch_size=batch_size, capacity=capacity)\n",
    "    \n",
    "    g_loss, d_loss = get_loss(image_batch, cartoon_batch)\n",
    "    g_train_opt, d_train_opt = get_optimizer(g_loss, d_loss, beta1, learning_rate)\n",
    "    \n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    # model_file=tf.train.latest_checkpoint('../input/cycel-apple-to-orange')\n",
    "    with tf.Session() as sess:\n",
    "        # saver.restore(sess, model_file)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "        # 迭代epoch\n",
    "        \n",
    "        for e in range(steps):\n",
    "                # run optimizer\n",
    "            _ = sess.run(d_train_opt)\n",
    "            _ = sess.run(g_train_opt)\n",
    "                \n",
    "            if e % 100 == 0:\n",
    "                # saver.save(sess,'./less96', global_step = e)\n",
    "                train_loss_d = d_loss.eval()\n",
    "                train_loss_g = g_loss.eval()\n",
    "                losses.append((train_loss_d, train_loss_g))\n",
    "                    # 显示图片\n",
    "                samples = show_generator_output(sess, image_batch)\n",
    "                plot_images(samples)\n",
    "                print(\"Epoch {}/{}....\".format(e+1, steps), \n",
    "                      \"Discriminator Loss: {:.4f}....\".format(train_loss_d),\n",
    "                      \"Generator Loss: {:.4f}....\". format(train_loss_g))\n",
    "        saver.save(sess,'../tf_saver_files/class_11_of_cycleGAN/generator.ckpt',global_step = steps)\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-3fef00bbdc15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-50dd7ecc8dc5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mg_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcartoon_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mg_train_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_train_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-746654c8cc7a>\u001b[0m in \u001b[0;36mget_optimizer\u001b[1;34m(g_loss, d_loss, g_loss_tran, learning_rate)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUPDATE_OPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mg_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mg_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0md_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mg_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_opt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         grad_loss=grad_loss)\n\u001b[0m\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[1;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgate_gradients\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGATE_OP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m         colocate_gradients_with_ops=colocate_gradients_with_ops)\n\u001b[0m\u001b[0;32m    513\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgate_gradients\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGATE_GRAPH\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m       \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m    662\u001b[0m     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,\n\u001b[0;32m    663\u001b[0m                             \u001b[0mgate_gradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m                             unconnected_gradients)\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[0;32m    954\u001b[0m                 \u001b[0mout_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloop_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZerosLike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 956\u001b[1;33m                 \u001b[0mout_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZerosLikeOutsideLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    957\u001b[0m           \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_grad\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m             \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mZerosLikeOutsideLoop\u001b[1;34m(op, index)\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1457\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_resource_variable_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1459\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m     \u001b[0mop_ctxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_control_flow_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mzeros_like\u001b[1;34m(tensor, dtype, name, optimize)\u001b[0m\n\u001b[0;32m   1848\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mall\u001b[0m \u001b[0melements\u001b[0m \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[0mzero\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m   \"\"\"\n\u001b[1;32m-> 1850\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mzeros_like_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mzeros_like_impl\u001b[1;34m(tensor, dtype, name, optimize)\u001b[0m\n\u001b[0;32m   1908\u001b[0m           shape_internal(tensor, optimize=optimize), dtype=dtype, name=name)\n\u001b[0;32m   1909\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1910\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mzeros_like\u001b[1;34m(x, name)\u001b[0m\n\u001b[0;32m  13548\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13549\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m> 13550\u001b[1;33m         \"ZerosLike\", x=x, name=name)\n\u001b[0m\u001b[0;32m  13551\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13552\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    789\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3300\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3302\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1823\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\SoftWare_Installing\\Anaconda3\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1634\u001b[0m   op_desc = c_api.TF_NewOperation(graph._c_graph,\n\u001b[0;32m   1635\u001b[0m                                   \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1636\u001b[1;33m                                   compat.as_str(node_def.name))\n\u001b[0m\u001b[0;32m   1637\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m     \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_SetDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
